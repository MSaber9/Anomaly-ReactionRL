{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE_RL_awid.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcamfer/Anomaly-ReactionRL/blob/master/Notebooks/AE_RL_awid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xRc2RK-glYlb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#AE-RL for AWID"
      ]
    },
    {
      "metadata": {
        "id": "FEFO4mGvbnlF",
        "colab_type": "code",
        "outputId": "eb61137e-b47b-4278-8a86-ce194b5e22b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "import json\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score,recall_score, precision_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "82_fPY7rlX-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using dataset from google drive"
      ]
    },
    {
      "metadata": {
        "id": "wWKDizH_ZqiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFrMypLUaXK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset path"
      ]
    },
    {
      "metadata": {
        "id": "G54yYRK3bhde",
        "colab_type": "code",
        "outputId": "7f7a51e1-2247-4128-d358-2b4f4165004d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!tar -xvzf \"/content/gdrive/My Drive/my_datasets/AWID-CLS-R-Trn.tar.gz\"\n",
        "!tar -xvzf \"/content/gdrive/My Drive/my_datasets/AWID-CLS-R-Tst.tar.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AWID-CLS-R-Trn/\n",
            "AWID-CLS-R-Trn/1\n",
            "AWID-CLS-R-Tst/\n",
            "AWID-CLS-R-Tst/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xHxe_kZtiQcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir splitted\n",
        "\n",
        "train_path = \"splitted/train.csv\"\n",
        "test_path = \"splitted/test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wUy5UJdSftEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "names = [\"frame.interface_id\" ,\"frame.dlt\" ,\"frame.offset_shift\",\"frame.time_epoch\" ,\n",
        "        \"frame.time_delta\" ,\"frame.time_delta_displayed\" ,\"frame.time_relative\" ,\n",
        "         \"frame.len\" ,\"frame.cap_len\" ,\"frame.marked\" ,\"frame.ignored\" ,\"radiotap.version\" ,\n",
        "    \"radiotap.pad\" ,\"radiotap.length\" ,\"radiotap.present.tsft\" ,\"radiotap.present.flags\" ,\"radiotap.present.rate\" ,\n",
        "    \"radiotap.present.channel\" ,\"radiotap.present.fhss\" ,\"radiotap.present.dbm_antsignal\" ,\"radiotap.present.dbm_antnoise\" ,\n",
        "    \"radiotap.present.lock_quality\" ,\"radiotap.present.tx_attenuation\" ,\"radiotap.present.db_tx_attenuation\" ,\n",
        "    \"radiotap.present.dbm_tx_power\" ,\"radiotap.present.antenna\" ,\"radiotap.present.db_antsignal\" ,\"radiotap.present.db_antnoise\" ,\n",
        "    \"radiotap.present.rxflags\" ,\"radiotap.present.xchannel\" ,\"radiotap.present.mcs\" ,\"radiotap.present.ampdu\" ,\"radiotap.present.vht\" ,\n",
        "    \"radiotap.present.reserved\" ,\"radiotap.present.rtap_ns\" ,\"radiotap.present.vendor_ns\" ,\"radiotap.present.ext\" ,\n",
        "    \"radiotap.mactime\" ,\"radiotap.flags.cfp\" ,\"radiotap.flags.preamble\" ,\"radiotap.flags.wep\" ,\"radiotap.flags.frag\" ,\n",
        "    \"radiotap.flags.fcs\" ,\"radiotap.flags.datapad\" ,\"radiotap.flags.badfcs\" ,\"radiotap.flags.shortgi\" ,\"radiotap.datarate\" ,\n",
        "    \"radiotap.channel.freq\" ,\"radiotap.channel.type.turbo\" ,\"radiotap.channel.type.cck\" ,\"radiotap.channel.type.ofdm\" ,\n",
        "    \"radiotap.channel.type.2ghz\" ,\"radiotap.channel.type.5ghz\" ,\"radiotap.channel.type.passive\" ,\"radiotap.channel.type.dynamic\" ,\n",
        "    \"ra√ádiotap.channel.type.gfsk\" ,\"radiotap.channel.type.gsm\" ,\"radiotap.channel.type.sturbo\" ,\"radiotap.channel.type.half\" ,\n",
        "    \"radiotap.channel.type.quarter\" ,\"radiotap.dbm_antsignal\" ,\"radiotap.antenna\" ,\"radiotap.rxflags.badplcp\" ,\"wlan.fc.type_subtype\" ,\n",
        "    \"wlan.fc.version\" ,\"wlan.fc.type\" ,\"wlan.fc.subtype\" ,\"wlan.fc.ds\" ,\"wlan.fc.frag\" ,\"wlan.fc.retry\" ,\"wlan.fc.pwrmgt\" ,\n",
        "    \"wlan.fc.moredata\" ,\"wlan.fc.protected\" ,\"wlan.fc.order\" ,\"wlan.duration\" ,\"wlan.ra\" ,\"wlan.da\" ,\"wlan.ta\" ,\"wlan.sa\" ,\n",
        "    \"wlan.bssid\" ,\"wlan.frag\" ,\"wlan.seq\" ,\"wlan.bar.type\" ,\"wlan.ba.control.ackpolicy\" ,\"wlan.ba.control.multitid\" ,\n",
        "    \"wlan.ba.control.cbitmap\" ,\"wlan.bar.compressed.tidinfo\" ,\"wlan.ba.bm\" ,\"wlan.fcs_good\" ,\"wlan_mgt.fixed.capabilities.ess\" ,\n",
        "    \"wlan_mgt.fixed.capabilities.ibss\" ,\"wlan_mgt.fixed.capabilities.cfpoll.ap\" ,\"wlan_mgt.fixed.capabilities.privacy\" ,\n",
        "    \"wlan_mgt.fixed.capabilities.preamble\" ,\"wlan_mgt.fixed.capabilities.pbcc\" ,\"wlan_mgt.fixed.capabilities.agility\" ,\n",
        "    \"wlan_mgt.fixed.capabilities.spec_man\" ,\"wlan_mgt.fixed.capabilities.short_slot_time\" ,\"wlan_mgt.fixed.capabilities.apsd\" ,\n",
        "    \"wlan_mgt.fixed.capabilities.radio_measurement\" ,\"wlan_mgt.fixed.capabilities.dsss_ofdm\" ,\"wlan_mgt.fixed.capabilities.del_blk_ack\" ,\n",
        "    \"wlan_mgt.fixed.capabilities.imm_blk_ack\" ,\"wlan_mgt.fixed.listen_ival\" ,\"wlan_mgt.fixed.current_ap\" ,\"wlan_mgt.fixed.status_code\" ,\n",
        "    \"wlan_mgt.fixed.timestamp\" ,\"wlan_mgt.fixed.beacon\" ,\"wlan_mgt.fixed.aid\" ,\"wlan_mgt.fixed.reason_code\" ,\"wlan_mgt.fixed.auth.alg\" ,\n",
        "    \"wlan_mgt.fixed.auth_seq\" ,\"wlan_mgt.fixed.category_code\" ,\"wlan_mgt.fixed.htact\" ,\"wlan_mgt.fixed.chanwidth\" ,\n",
        "    \"wlan_mgt.fixed.fragment\" ,\"wlan_mgt.fixed.sequence\" ,\"wlan_mgt.tagged.all\" ,\"wlan_mgt.ssid\" ,\"wlan_mgt.ds.current_channel\" ,\n",
        "    \"wlan_mgt.tim.dtim_count\" ,\"wlan_mgt.tim.dtim_period\" ,\"wlan_mgt.tim.bmapctl.multicast\" ,\"wlan_mgt.tim.bmapctl.offset\" ,\n",
        "    \"wlan_mgt.country_info.environment\" ,\"wlan_mgt.rsn.version\" ,\"wlan_mgt.rsn.gcs.type\" ,\"wlan_mgt.rsn.pcs.count\" ,\n",
        "    \"wlan_mgt.rsn.akms.count\" ,\"wlan_mgt.rsn.akms.type\" ,\"wlan_mgt.rsn.capabilities.preauth\" ,\"wlan_mgt.rsn.capabilities.no_pairwise\" ,\n",
        "    \"wlan_mgt.rsn.capabilities.ptksa_replay_counter\" ,\"wlan_mgt.rsn.capabilities.gtksa_replay_counter\" ,\"wlan_mgt.rsn.capabilities.mfpr\" ,\n",
        "    \"wlan_mgt.rsn.capabilities.mfpc\" ,\"wlan_mgt.rsn.capabilities.peerkey\" ,\"wlan_mgt.tcprep.trsmt_pow\" ,\"wlan_mgt.tcprep.link_mrg\" ,\n",
        "    \"wlan.wep.iv\" ,\"wlan.wep.key\" ,\"wlan.wep.icv\" ,\"wlan.tkip.extiv\" ,\"wlan.ccmp.extiv\" ,\"wlan.qos.tid\" ,\"wlan.qos.priority\" ,\n",
        "    \"wlan.qos.eosp\" ,\"wlan.qos.ack\" ,\"wlan.qos.amsdupresent\" ,\"wlan.qos.buf_state_indicated\" ,\"wlan.qos.bit4\" ,\n",
        "    \"wlan.qos.txop_dur_req\" ,\"wlan.qos.buf_state_indicated\" ,\"data.len\" ,\"class\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-DUODMU5ltL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the unique features used"
      ]
    },
    {
      "metadata": {
        "id": "rBeqlxB1Hptv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "colnames_good_df = pd.read_csv(\"/content/gdrive/My Drive/my_datasets/Awid/colnames_good.csv\",names=[\"colnames\"])\n",
        "colnames_good = colnames_good_df['colnames'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJlWn4BtcZx4",
        "colab_type": "code",
        "outputId": "3848c1a0-d410-4d7f-a6e2-1515ea6d1a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"AWID-CLS-R-Trn/1\", names=names, usecols=colnames_good)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
            "  return _read(filepath_or_buffer, kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "p54-dQO336F7",
        "colab_type": "code",
        "outputId": "0c9845ac-1b96-42a7-b00b-691c862384a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"AWID-CLS-R-Tst/1\", names=names, usecols=colnames_good)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
            "  return _read(filepath_or_buffer, kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CFVq5S4xBcXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "JoujDQHTQ2OD",
        "colab_type": "code",
        "outputId": "125f9df0-dc42-4e14-9316-9e5e9ba05ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1795575, 49)\n",
            "(575643, 49)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54infFBHBC0h",
        "colab_type": "code",
        "outputId": "7ca4ea16-2cda-424d-8d2f-8bd75cc6e80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "train_indx = train.shape[0]\n",
        "\n",
        "df = train.append(test, ignore_index=True)\n",
        "\n",
        "# Remove labels column\n",
        "labels = df['class']\n",
        "df = df.drop('class',axis=1)\n",
        "\n",
        "print(\"Dataset without labels shape:{}\".format(df.shape))\n",
        "\n",
        "\n",
        "# Processing categorical and numerical columns\n",
        "num_cols = list(df._get_numeric_data().columns)\n",
        "cat_cols = list(set(df.columns)-set(num_cols))\n",
        "\n",
        "\n",
        "# Normalization of the df\n",
        "#log_cols = self.df.filter(like='logarithm').columns\n",
        "#nat_cols =  list(set(self.df.columns)-set(log_cols))\n",
        "#self.df.reset_index()\n",
        "df[num_cols] = (df[num_cols]-df[num_cols].min())/(df[num_cols].max()-df[num_cols].min())\n",
        "\n",
        "\n",
        "for name_col in cat_cols:\n",
        "    df = pd.concat([df.drop(name_col, axis=1), pd.get_dummies(df[name_col],prefix=name_col)], axis=1)\n",
        "\n",
        "print(\"Dataset with categoricals encoded:{}\".format(df.shape))\n",
        "\n",
        "# If na max and min = 0 so delete column\n",
        "df = df.dropna(axis=1)\n",
        "\n",
        "# Delete row\n",
        "#self.df = self.df.dropna(axis=0)\n",
        "print(\"Dataset dropping nan:{}\".format(df.shape))\n",
        "\n",
        "# Add labels again\n",
        "df['labels']=labels\n",
        "\n",
        "\n",
        "print(\"Dataset with labels again:{}\".format(df.shape))\n",
        "\n",
        "\n",
        "train = df.iloc[:train_indx]\n",
        "test = df.iloc[train_indx:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset without labels shape:(2371218, 48)\n",
            "Dataset with categoricals encoded:(2371218, 70)\n",
            "Dataset dropping nan:(2371218, 46)\n",
            "Dataset with labels again:(2371218, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "doAyeUMdxL7G",
        "colab_type": "code",
        "outputId": "942ff468-4099-4141-d268-0e128ac3f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1795575, 47)\n",
            "(575643, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pj8-UQ5_ypLe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.to_csv(train_path,index=False)\n",
        "test.to_csv(test_path,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5H_JmUZtceEN",
        "colab_type": "code",
        "outputId": "09162ecf-a529-4655-a338-34a88f88bfc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1795575 entries, 0 to 1795574\n",
            "Data columns (total 47 columns):\n",
            "frame.time_epoch                        float64\n",
            "frame.time_delta                        float64\n",
            "frame.time_delta_displayed              float64\n",
            "frame.time_relative                     float64\n",
            "frame.len                               float64\n",
            "frame.cap_len                           float64\n",
            "radiotap.length                         float64\n",
            "radiotap.present.tsft                   float64\n",
            "radiotap.present.flags                  float64\n",
            "radiotap.present.channel                float64\n",
            "radiotap.present.dbm_antsignal          float64\n",
            "radiotap.present.antenna                float64\n",
            "radiotap.present.rxflags                float64\n",
            "radiotap.datarate                       float64\n",
            "wlan.fc.type                            float64\n",
            "wlan.fc.subtype                         float64\n",
            "wlan.fc.frag                            float64\n",
            "wlan.fc.retry                           float64\n",
            "wlan.fc.pwrmgt                          float64\n",
            "wlan.fc.moredata                        float64\n",
            "wlan.fc.protected                       float64\n",
            "radiotap.present.reserved_0x00000000    uint8\n",
            "wlan.fc.type_subtype_0x00               uint8\n",
            "wlan.fc.type_subtype_0x01               uint8\n",
            "wlan.fc.type_subtype_0x02               uint8\n",
            "wlan.fc.type_subtype_0x03               uint8\n",
            "wlan.fc.type_subtype_0x04               uint8\n",
            "wlan.fc.type_subtype_0x05               uint8\n",
            "wlan.fc.type_subtype_0x08               uint8\n",
            "wlan.fc.type_subtype_0x0a               uint8\n",
            "wlan.fc.type_subtype_0x0b               uint8\n",
            "wlan.fc.type_subtype_0x0c               uint8\n",
            "wlan.fc.type_subtype_0x0d               uint8\n",
            "wlan.fc.type_subtype_0x18               uint8\n",
            "wlan.fc.type_subtype_0x19               uint8\n",
            "wlan.fc.type_subtype_0x1a               uint8\n",
            "wlan.fc.type_subtype_0x1b               uint8\n",
            "wlan.fc.type_subtype_0x1c               uint8\n",
            "wlan.fc.type_subtype_0x1d               uint8\n",
            "wlan.fc.type_subtype_0x20               uint8\n",
            "wlan.fc.type_subtype_0x24               uint8\n",
            "wlan.fc.type_subtype_0x28               uint8\n",
            "wlan.fc.type_subtype_0x2c               uint8\n",
            "wlan.fc.ds_0x00                         uint8\n",
            "wlan.fc.ds_0x01                         uint8\n",
            "wlan.fc.ds_0x02                         uint8\n",
            "labels                                  object\n",
            "dtypes: float64(21), object(1), uint8(25)\n",
            "memory usage: 344.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7GmfCWBXc0du",
        "colab_type": "code",
        "outputId": "d2d52e71-e45c-401c-bafd-e086e3961510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame.time_epoch</th>\n",
              "      <th>frame.time_delta</th>\n",
              "      <th>frame.time_delta_displayed</th>\n",
              "      <th>frame.time_relative</th>\n",
              "      <th>frame.len</th>\n",
              "      <th>frame.cap_len</th>\n",
              "      <th>radiotap.length</th>\n",
              "      <th>radiotap.present.tsft</th>\n",
              "      <th>radiotap.present.flags</th>\n",
              "      <th>radiotap.present.channel</th>\n",
              "      <th>...</th>\n",
              "      <th>wlan.fc.type_subtype_0x1b</th>\n",
              "      <th>wlan.fc.type_subtype_0x1c</th>\n",
              "      <th>wlan.fc.type_subtype_0x1d</th>\n",
              "      <th>wlan.fc.type_subtype_0x20</th>\n",
              "      <th>wlan.fc.type_subtype_0x24</th>\n",
              "      <th>wlan.fc.type_subtype_0x28</th>\n",
              "      <th>wlan.fc.type_subtype_0x2c</th>\n",
              "      <th>wlan.fc.ds_0x00</th>\n",
              "      <th>wlan.fc.ds_0x01</th>\n",
              "      <th>wlan.fc.ds_0x02</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "      <td>1.795575e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.312808e-01</td>\n",
              "      <td>5.723385e-03</td>\n",
              "      <td>5.723385e-03</td>\n",
              "      <td>5.195880e-01</td>\n",
              "      <td>3.106633e-01</td>\n",
              "      <td>3.106633e-01</td>\n",
              "      <td>9.995656e-01</td>\n",
              "      <td>9.995656e-01</td>\n",
              "      <td>9.995656e-01</td>\n",
              "      <td>9.995656e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>7.100789e-04</td>\n",
              "      <td>1.999694e-02</td>\n",
              "      <td>2.321902e-01</td>\n",
              "      <td>1.302697e-01</td>\n",
              "      <td>1.033875e-02</td>\n",
              "      <td>3.683216e-01</td>\n",
              "      <td>5.524693e-04</td>\n",
              "      <td>4.905175e-01</td>\n",
              "      <td>1.369093e-01</td>\n",
              "      <td>3.725731e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.072683e-01</td>\n",
              "      <td>1.821356e-02</td>\n",
              "      <td>1.821356e-02</td>\n",
              "      <td>2.409856e-01</td>\n",
              "      <td>4.345912e-01</td>\n",
              "      <td>4.345912e-01</td>\n",
              "      <td>2.083777e-02</td>\n",
              "      <td>2.083777e-02</td>\n",
              "      <td>2.083777e-02</td>\n",
              "      <td>2.083777e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>2.663785e-02</td>\n",
              "      <td>1.399895e-01</td>\n",
              "      <td>4.222298e-01</td>\n",
              "      <td>3.366000e-01</td>\n",
              "      <td>1.011527e-01</td>\n",
              "      <td>4.823494e-01</td>\n",
              "      <td>2.349818e-02</td>\n",
              "      <td>4.999102e-01</td>\n",
              "      <td>3.437517e-01</td>\n",
              "      <td>4.834899e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.271072e-01</td>\n",
              "      <td>8.993242e-04</td>\n",
              "      <td>8.993242e-04</td>\n",
              "      <td>2.855550e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.352776e-01</td>\n",
              "      <td>1.567394e-03</td>\n",
              "      <td>1.567394e-03</td>\n",
              "      <td>5.285670e-01</td>\n",
              "      <td>4.705882e-02</td>\n",
              "      <td>4.705882e-02</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.121584e-01</td>\n",
              "      <td>3.177612e-03</td>\n",
              "      <td>3.177612e-03</td>\n",
              "      <td>7.012848e-01</td>\n",
              "      <td>9.882353e-01</td>\n",
              "      <td>9.882353e-01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.451235e-01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows √ó 46 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       frame.time_epoch  frame.time_delta  frame.time_delta_displayed  \\\n",
              "count      1.795575e+06      1.795575e+06                1.795575e+06   \n",
              "mean       2.312808e-01      5.723385e-03                5.723385e-03   \n",
              "std        1.072683e-01      1.821356e-02                1.821356e-02   \n",
              "min        0.000000e+00      0.000000e+00                0.000000e+00   \n",
              "25%        1.271072e-01      8.993242e-04                8.993242e-04   \n",
              "50%        2.352776e-01      1.567394e-03                1.567394e-03   \n",
              "75%        3.121584e-01      3.177612e-03                3.177612e-03   \n",
              "max        4.451235e-01      1.000000e+00                1.000000e+00   \n",
              "\n",
              "       frame.time_relative     frame.len  frame.cap_len  radiotap.length  \\\n",
              "count         1.795575e+06  1.795575e+06   1.795575e+06     1.795575e+06   \n",
              "mean          5.195880e-01  3.106633e-01   3.106633e-01     9.995656e-01   \n",
              "std           2.409856e-01  4.345912e-01   4.345912e-01     2.083777e-02   \n",
              "min           0.000000e+00  0.000000e+00   0.000000e+00     0.000000e+00   \n",
              "25%           2.855550e-01  0.000000e+00   0.000000e+00     1.000000e+00   \n",
              "50%           5.285670e-01  4.705882e-02   4.705882e-02     1.000000e+00   \n",
              "75%           7.012848e-01  9.882353e-01   9.882353e-01     1.000000e+00   \n",
              "max           1.000000e+00  1.000000e+00   1.000000e+00     1.000000e+00   \n",
              "\n",
              "       radiotap.present.tsft  radiotap.present.flags  \\\n",
              "count           1.795575e+06            1.795575e+06   \n",
              "mean            9.995656e-01            9.995656e-01   \n",
              "std             2.083777e-02            2.083777e-02   \n",
              "min             0.000000e+00            0.000000e+00   \n",
              "25%             1.000000e+00            1.000000e+00   \n",
              "50%             1.000000e+00            1.000000e+00   \n",
              "75%             1.000000e+00            1.000000e+00   \n",
              "max             1.000000e+00            1.000000e+00   \n",
              "\n",
              "       radiotap.present.channel       ...         wlan.fc.type_subtype_0x1b  \\\n",
              "count              1.795575e+06       ...                      1.795575e+06   \n",
              "mean               9.995656e-01       ...                      7.100789e-04   \n",
              "std                2.083777e-02       ...                      2.663785e-02   \n",
              "min                0.000000e+00       ...                      0.000000e+00   \n",
              "25%                1.000000e+00       ...                      0.000000e+00   \n",
              "50%                1.000000e+00       ...                      0.000000e+00   \n",
              "75%                1.000000e+00       ...                      0.000000e+00   \n",
              "max                1.000000e+00       ...                      1.000000e+00   \n",
              "\n",
              "       wlan.fc.type_subtype_0x1c  wlan.fc.type_subtype_0x1d  \\\n",
              "count               1.795575e+06               1.795575e+06   \n",
              "mean                1.999694e-02               2.321902e-01   \n",
              "std                 1.399895e-01               4.222298e-01   \n",
              "min                 0.000000e+00               0.000000e+00   \n",
              "25%                 0.000000e+00               0.000000e+00   \n",
              "50%                 0.000000e+00               0.000000e+00   \n",
              "75%                 0.000000e+00               0.000000e+00   \n",
              "max                 1.000000e+00               1.000000e+00   \n",
              "\n",
              "       wlan.fc.type_subtype_0x20  wlan.fc.type_subtype_0x24  \\\n",
              "count               1.795575e+06               1.795575e+06   \n",
              "mean                1.302697e-01               1.033875e-02   \n",
              "std                 3.366000e-01               1.011527e-01   \n",
              "min                 0.000000e+00               0.000000e+00   \n",
              "25%                 0.000000e+00               0.000000e+00   \n",
              "50%                 0.000000e+00               0.000000e+00   \n",
              "75%                 0.000000e+00               0.000000e+00   \n",
              "max                 1.000000e+00               1.000000e+00   \n",
              "\n",
              "       wlan.fc.type_subtype_0x28  wlan.fc.type_subtype_0x2c  wlan.fc.ds_0x00  \\\n",
              "count               1.795575e+06               1.795575e+06     1.795575e+06   \n",
              "mean                3.683216e-01               5.524693e-04     4.905175e-01   \n",
              "std                 4.823494e-01               2.349818e-02     4.999102e-01   \n",
              "min                 0.000000e+00               0.000000e+00     0.000000e+00   \n",
              "25%                 0.000000e+00               0.000000e+00     0.000000e+00   \n",
              "50%                 0.000000e+00               0.000000e+00     0.000000e+00   \n",
              "75%                 1.000000e+00               0.000000e+00     1.000000e+00   \n",
              "max                 1.000000e+00               1.000000e+00     1.000000e+00   \n",
              "\n",
              "       wlan.fc.ds_0x01  wlan.fc.ds_0x02  \n",
              "count     1.795575e+06     1.795575e+06  \n",
              "mean      1.369093e-01     3.725731e-01  \n",
              "std       3.437517e-01     4.834899e-01  \n",
              "min       0.000000e+00     0.000000e+00  \n",
              "25%       0.000000e+00     0.000000e+00  \n",
              "50%       0.000000e+00     0.000000e+00  \n",
              "75%       0.000000e+00     1.000000e+00  \n",
              "max       1.000000e+00     1.000000e+00  \n",
              "\n",
              "[8 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "WXzr-11HTlU6",
        "colab_type": "code",
        "outputId": "9cd39e9d-a4ee-4647-b9de-77645ec547d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 575643 entries, 1795575 to 2371217\n",
            "Data columns (total 47 columns):\n",
            "frame.time_epoch                        575643 non-null float64\n",
            "frame.time_delta                        575643 non-null float64\n",
            "frame.time_delta_displayed              575643 non-null float64\n",
            "frame.time_relative                     575643 non-null float64\n",
            "frame.len                               575643 non-null float64\n",
            "frame.cap_len                           575643 non-null float64\n",
            "radiotap.length                         575643 non-null float64\n",
            "radiotap.present.tsft                   575643 non-null float64\n",
            "radiotap.present.flags                  575643 non-null float64\n",
            "radiotap.present.channel                575643 non-null float64\n",
            "radiotap.present.dbm_antsignal          575643 non-null float64\n",
            "radiotap.present.antenna                575643 non-null float64\n",
            "radiotap.present.rxflags                575643 non-null float64\n",
            "radiotap.datarate                       575643 non-null float64\n",
            "wlan.fc.type                            575643 non-null float64\n",
            "wlan.fc.subtype                         575643 non-null float64\n",
            "wlan.fc.frag                            575643 non-null float64\n",
            "wlan.fc.retry                           575643 non-null float64\n",
            "wlan.fc.pwrmgt                          575643 non-null float64\n",
            "wlan.fc.moredata                        575643 non-null float64\n",
            "wlan.fc.protected                       575643 non-null float64\n",
            "radiotap.present.reserved_0x00000000    575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x00               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x01               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x02               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x03               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x04               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x05               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x08               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x0a               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x0b               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x0c               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x0d               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x18               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x19               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x1a               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x1b               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x1c               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x1d               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x20               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x24               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x28               575643 non-null uint8\n",
            "wlan.fc.type_subtype_0x2c               575643 non-null uint8\n",
            "wlan.fc.ds_0x00                         575643 non-null uint8\n",
            "wlan.fc.ds_0x01                         575643 non-null uint8\n",
            "wlan.fc.ds_0x02                         575643 non-null uint8\n",
            "labels                                  575643 non-null object\n",
            "dtypes: float64(21), object(1), uint8(25)\n",
            "memory usage: 110.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2kk6Nys7QHHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_cols = list(train._get_numeric_data().columns)\n",
        "cat_cols = list(set(train.columns)-set(num_cols))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5k4zfKxRQI9G",
        "colab_type": "code",
        "outputId": "0b67bb6d-7f84-454b-a74a-fc799fe67c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(num_cols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "VnVwRFWMdCEV",
        "colab_type": "code",
        "outputId": "2bf1e200-e67f-4175-9ff4-1d026d7b195c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1795575, 47)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "YKA8QTFrb1ZR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class data_cls:\n",
        "    def __init__(self,train_test):\n",
        "        \n",
        "       \n",
        "        self.index = 0\n",
        "        if(train_test==\"train\"):\n",
        "          self.train_path = train_path\n",
        "        else:\n",
        "          self.train_path = test_path\n",
        "       \n",
        "   \n",
        "        self.df = pd.read_csv(self.train_path,sep=',')\n",
        "    \n",
        "\n",
        "         # One hot encoding for labels\n",
        "        self.df = pd.concat([self.df.drop('labels', axis=1),\n",
        "                pd.get_dummies(self.df['labels'])], axis=1)\n",
        "        \n",
        "\n",
        "        self.attack_types = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "        self.all_attack_types = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "        self.attack_names = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "        \n",
        "\n",
        "        self.attack_map =   { 'normal': 'normal','flooding': 'flooding', 'injection':'injection', 'impersonation':'impersonation'}\n",
        "        \n",
        "        self.all_attack_names = list(self.attack_map.keys())\n",
        "\n",
        "        self.df = self.df.sample(frac=1)\n",
        "\n",
        "\n",
        "    def get_shape(self):\n",
        "              \n",
        "        self.data_shape = self.df.shape\n",
        "        # stata + labels\n",
        "        return self.data_shape\n",
        "    \n",
        "    ''' Get n-rows from loaded data \n",
        "        The dataset must be loaded in RAM\n",
        "    '''\n",
        "    def get_batch(self,batch_size=100):\n",
        "                \n",
        "        # Read the df rows\n",
        "        indexes = list(range(self.index,self.index+batch_size))    \n",
        "        if max(indexes)>self.data_shape[0]-1:\n",
        "            dif = max(indexes)-self.data_shape[0]\n",
        "            indexes[len(indexes)-dif-1:len(indexes)] = list(range(dif+1))\n",
        "            self.index=batch_size-dif\n",
        "            batch = self.df.iloc[indexes]\n",
        "        else: \n",
        "            batch = self.df.iloc[indexes]\n",
        "            self.index += batch_size    \n",
        "            \n",
        "        labels = batch[self.attack_names]\n",
        "        \n",
        "        batch = batch.drop(self.all_attack_names,axis=1)\n",
        "            \n",
        "        return batch,labels\n",
        "    \n",
        "    def get_full(self):\n",
        "              \n",
        "        labels = self.df[self.attack_names]\n",
        "        \n",
        "        batch = self.df.drop(self.all_attack_names,axis=1)\n",
        "        \n",
        "\n",
        "        return batch,labels\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DbMxSATehU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Huber loss function        \n",
        "def huber_loss(y_true, y_pred, clip_value=1):\n",
        "    # Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n",
        "    # https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
        "    # for details.\n",
        "    assert clip_value > 0.\n",
        "\n",
        "    x = y_true - y_pred\n",
        "    if np.isinf(clip_value):\n",
        "        # Spacial case for infinity since Tensorflow does have problems\n",
        "        # if we compare `K.abs(x) < np.inf`.\n",
        "        return .5 * K.square(x)\n",
        "\n",
        "    condition = K.abs(x) < clip_value\n",
        "    squared_loss = .5 * K.square(x)\n",
        "    linear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n",
        "    if K.backend() == 'tensorflow':\n",
        "        import tensorflow as tf\n",
        "        if hasattr(tf, 'select'):\n",
        "            return tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n",
        "        else:\n",
        "            return tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n",
        "    elif K.backend() == 'theano':\n",
        "        from theano import tensor as T\n",
        "        return T.switch(condition, squared_loss, linear_loss)\n",
        "    else:\n",
        "        raise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n",
        "\n",
        "# Needed for keras huber_loss locate\n",
        "import keras.losses\n",
        "keras.losses.huber_loss = huber_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XkKb6WaesLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QNetwork():\n",
        "    \"\"\"\n",
        "    Q-Network Estimator\n",
        "    Represents the global model for the table\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,obs_size,num_actions,hidden_size = 100,\n",
        "                 hidden_layers = 1,learning_rate=.2):\n",
        "        \"\"\"\n",
        "        Initialize the network with the provided shape\n",
        "        \"\"\"\n",
        "        self.obs_size = obs_size\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        # Network arquitecture\n",
        "        self.model = Sequential()\n",
        "        # Add imput layer\n",
        "        self.model.add(Dense(hidden_size, input_shape=(obs_size,),\n",
        "                             activation='relu'))\n",
        "        # Add hidden layers\n",
        "        for layers in range(hidden_layers):\n",
        "            self.model.add(Dense(hidden_size, activation='relu'))\n",
        "        # Add output layer    \n",
        "        self.model.add(Dense(num_actions))\n",
        "        \n",
        "        #optimizer = optimizers.SGD(learning_rate)\n",
        "        # optimizer = optimizers.Adam(alpha=learning_rate)\n",
        "        optimizer = optimizers.Adam(0.00025)\n",
        "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
        "        \n",
        "        # Compilation of the model with optimizer and loss\n",
        "        self.model.compile(loss=huber_loss,optimizer=optimizer)\n",
        "\n",
        "    def predict(self,state,batch_size=1):\n",
        "        \"\"\"\n",
        "        Predicts action values.\n",
        "        \"\"\"\n",
        "        return self.model.predict(state,batch_size=batch_size)\n",
        "\n",
        "    def update(self, states, q):\n",
        "        \"\"\"\n",
        "        Updates the estimator with the targets.\n",
        "\n",
        "        Args:\n",
        "          states: Target states\n",
        "          q: Estimated values\n",
        "\n",
        "        Returns:\n",
        "          The calculated loss on the batch.\n",
        "        \"\"\"\n",
        "        loss = self.model.train_on_batch(states, q)\n",
        "        return loss\n",
        "    \n",
        "    def copy_model(model):\n",
        "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
        "        model.save('tmp_model')\n",
        "        return keras.models.load_model('tmp_model')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-u8-r77Jexf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Policy interface\n",
        "class Policy:\n",
        "    def __init__(self, num_actions, estimator):\n",
        "        self.num_actions = num_actions\n",
        "        self.estimator = estimator\n",
        "    \n",
        "class Epsilon_greedy(Policy):\n",
        "    def __init__(self,estimator ,num_actions ,epsilon,min_epsilon,decay_rate, epoch_length):\n",
        "        Policy.__init__(self, num_actions, estimator)\n",
        "        self.name = \"Epsilon Greedy\"\n",
        "        \n",
        "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
        "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
        "            sys.exit(0)\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.actions = list(range(num_actions))\n",
        "        self.step_counter = 0\n",
        "        self.epoch_length = epoch_length\n",
        "        self.decay_rate = decay_rate\n",
        "        \n",
        "        #if epsilon is up 0.1, it will be decayed over time\n",
        "        if self.epsilon > 0.01:\n",
        "            self.epsilon_decay = True\n",
        "        else:\n",
        "            self.epsilon_decay = False\n",
        "    \n",
        "    def get_actions(self,states):\n",
        "        # get next action\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            actions = np.random.randint(0, self.num_actions,states.shape[0])\n",
        "        else:\n",
        "            self.Q = self.estimator.predict(states,states.shape[0])\n",
        "            actions = []\n",
        "            for row in range(self.Q.shape[0]):\n",
        "                best_actions = np.argwhere(self.Q[row] == np.amax(self.Q[row]))\n",
        "                actions.append(best_actions[np.random.choice(len(best_actions))].item())\n",
        "            \n",
        "        self.step_counter += 1 \n",
        "        # decay epsilon after each epoch\n",
        "        if self.epsilon_decay:\n",
        "            if self.step_counter % self.epoch_length == 0:\n",
        "                self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_rate**self.step_counter)\n",
        "            \n",
        "        return actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gBTfI8ke08v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Implements basic replay memory\"\"\"\n",
        "\n",
        "    def __init__(self, observation_size, max_size):\n",
        "        self.observation_size = observation_size\n",
        "        self.num_observed = 0\n",
        "        self.max_size = max_size\n",
        "        self.samples = {\n",
        "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
        "                                       dtype=np.float32).reshape(self.max_size,self.observation_size),\n",
        "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
        "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "               }\n",
        "\n",
        "    def observe(self, state, action, reward, done):\n",
        "        index = self.num_observed % self.max_size\n",
        "        self.samples['obs'][index, :] = state\n",
        "        self.samples['action'][index, :] = action\n",
        "        self.samples['reward'][index, :] = reward\n",
        "        self.samples['terminal'][index, :] = done\n",
        "\n",
        "        self.num_observed += 1\n",
        "\n",
        "    def sample_minibatch(self, minibatch_size):\n",
        "        max_index = min(self.num_observed, self.max_size) - 1\n",
        "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
        "\n",
        "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
        "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
        "\n",
        "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
        "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
        "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
        "\n",
        "        return (s, a, r, s_next, done)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXeEzOaIe32m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reinforcement learning Agent definition\n",
        "'''\n",
        "\n",
        "class Agent(object):  \n",
        "        \n",
        "    def __init__(self, actions,obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        self.actions = actions\n",
        "        self.num_actions = len(actions)\n",
        "        self.obs_size = obs_size\n",
        "        \n",
        "        self.epsilon = kwargs.get('epsilon', 1)\n",
        "        self.min_epsilon = kwargs.get('min_epsilon', .1)\n",
        "        self.gamma = kwargs.get('gamma', .001)\n",
        "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
        "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
        "        self.decay_rate = kwargs.get('decay_rate',0.99)\n",
        "        self.ExpRep = kwargs.get('ExpRep',True)\n",
        "        if self.ExpRep:\n",
        "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
        "        \n",
        "        self.ddqn_time = 100\n",
        "        self.ddqn_update = self.ddqn_time\n",
        "\n",
        "        \n",
        "        self.model_network = QNetwork(self.obs_size, self.num_actions,\n",
        "                                      kwargs.get('hidden_size', 100),\n",
        "                                      kwargs.get('hidden_layers',1),\n",
        "                                      kwargs.get('learning_rate',.2))\n",
        "        self.target_model_network = QNetwork(self.obs_size, self.num_actions,\n",
        "                                      kwargs.get('hidden_size', 100),\n",
        "                                      kwargs.get('hidden_layers',1),\n",
        "                                      kwargs.get('learning_rate',.2))\n",
        "        self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
        "        \n",
        "        if policy == \"EpsilonGreedy\":\n",
        "            self.policy = Epsilon_greedy(self.model_network,len(actions),\n",
        "                                         self.epsilon,self.min_epsilon,\n",
        "                                         self.decay_rate,self.epoch_length)\n",
        "        \n",
        "        \n",
        "    def learn(self, states, actions,next_states, rewards, done):\n",
        "        if self.ExpRep:\n",
        "            self.memory.observe(states, actions, rewards, done)\n",
        "        else:\n",
        "            self.states = states\n",
        "            self.actions = actions\n",
        "            self.next_states = next_states\n",
        "            self.rewards = rewards\n",
        "            self.done = done        \n",
        "    def update_model(self):\n",
        "        if self.ExpRep:\n",
        "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
        "        else:\n",
        "            states = self.states\n",
        "            rewards = self.rewards\n",
        "            next_states = self.next_states\n",
        "            actions = self.actions\n",
        "            done = self.done\n",
        "        \n",
        "        next_actions = []\n",
        "        # Compute Q targets\n",
        "#        Q_prime = self.model_network.predict(next_states,self.minibatch_size)\n",
        "        Q_prime = self.target_model_network.predict(next_states,self.minibatch_size)\n",
        "        # TODO: fix performance in this loop\n",
        "        for row in range(Q_prime.shape[0]):\n",
        "            best_next_actions = np.argwhere(Q_prime[row] == np.amax(Q_prime[row]))\n",
        "            next_actions.append(best_next_actions[np.random.choice(len(best_next_actions))].item())\n",
        "        sx = np.arange(len(next_actions))\n",
        "        # Compute Q(s,a)\n",
        "        Q = self.model_network.predict(states,self.minibatch_size)\n",
        "        # Q-learning update\n",
        "        # target = reward + gamma * max_a'{Q(next_state,next_action))}\n",
        "        targets = rewards.reshape(Q[sx,actions].shape) + \\\n",
        "                  self.gamma * Q[sx,next_actions] * \\\n",
        "                  (1-done.reshape(Q[sx,actions].shape))   \n",
        "        Q[sx,actions] = targets  \n",
        "        \n",
        "        loss = self.model_network.model.train_on_batch(states,Q)#inputs,targets        \n",
        "        \n",
        "        # timer to ddqn update\n",
        "        self.ddqn_update -= 1\n",
        "        if self.ddqn_update == 0:\n",
        "            self.ddqn_update = self.ddqn_time\n",
        "#            self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
        "            self.target_model_network.model.set_weights(self.model_network.model.get_weights()) \n",
        "        \n",
        "        return loss    \n",
        "\n",
        "    def act(self, state,policy):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-K4eEuDe-QE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzoWE1L8fCKP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DefenderAgent(Agent):      \n",
        "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", **kwargs)\n",
        "        \n",
        "    def act(self,states):\n",
        "        # Get actions under the policy\n",
        "        actions = self.policy.get_actions(states)\n",
        "        return actions\n",
        "    \n",
        "class AttackAgent(Agent):      \n",
        "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", **kwargs)\n",
        "        \n",
        "    def act(self,states):\n",
        "        # Get actions under the policy\n",
        "        actions = self.policy.get_actions(states)\n",
        "        return actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ojLCmmE3fCpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reinforcement learning Enviroment Definition\n",
        "'''\n",
        "class RLenv(data_cls):\n",
        "    def __init__(self,train_test,**kwargs):\n",
        "        data_cls.__init__(self,train_test)\n",
        "        self.data_shape = data_cls.get_shape(self)\n",
        "        self.batch_size = kwargs.get('batch_size',1) # experience replay -> batch = 1\n",
        "        self.iterations_episode = kwargs.get('iterations_episode',10)\n",
        "        \n",
        "\n",
        "    '''\n",
        "    _update_state: function to update the current state\n",
        "    Returns:\n",
        "        None\n",
        "    Modifies the self parameters involved in the state:\n",
        "        self.state and self.labels\n",
        "    Also modifies the true labels to get learning knowledge\n",
        "    '''\n",
        "    def _update_state(self):        \n",
        "        self.states,self.labels = data_cls.get_batch(self)\n",
        "        \n",
        "        # Update statistics\n",
        "        self.true_labels += np.sum(self.labels).values\n",
        "\n",
        "    '''\n",
        "    Returns:\n",
        "        + Observation of the enviroment\n",
        "    '''\n",
        "    def reset(self):\n",
        "        # Statistics\n",
        "        self.def_true_labels = np.zeros(len(self.attack_types),dtype=int)\n",
        "        self.def_estimated_labels = np.zeros(len(self.attack_types),dtype=int)\n",
        "        self.att_true_labels = np.zeros(len(self.attack_names),dtype=int)\n",
        "        \n",
        "        self.state_numb = 0\n",
        "        \n",
        "        self.states,self.labels = data_cls.get_batch(self,self.batch_size)\n",
        "        \n",
        "        self.total_reward = 0\n",
        "        self.steps_in_episode = 0\n",
        "        return self.states.values \n",
        "   \n",
        "    '''\n",
        "    Returns:\n",
        "        State: Next state for the game\n",
        "        Reward: Actual reward\n",
        "        done: If the game ends (no end in this case)\n",
        "    \n",
        "    In the adversarial enviroment, it's only needed to return the actual reward\n",
        "    '''    \n",
        "    def act(self,defender_actions,attack_actions):\n",
        "        # Clear previous rewards        \n",
        "        self.att_reward = np.zeros(len(attack_actions))       \n",
        "        self.def_reward = np.zeros(len(defender_actions))\n",
        "        \n",
        "        \n",
        "        attack = [self.attack_types.index(self.attack_map[self.attack_names[att]]) for att in attack_actions]\n",
        "        \n",
        "        self.def_reward = (np.asarray(defender_actions)==np.asarray(attack))*1\n",
        "        self.att_reward = (np.asarray(defender_actions)!=np.asarray(attack))*1\n",
        "\n",
        "         \n",
        "       \n",
        "        self.def_estimated_labels += np.bincount(defender_actions,minlength=len(self.attack_types))\n",
        "        # TODO\n",
        "        # list comprehension\n",
        "        \n",
        "        for act in attack_actions:\n",
        "            self.def_true_labels[self.attack_types.index(self.attack_map[self.attack_names[act]])] += 1\n",
        "        \n",
        "\n",
        "        # Get new state and new true values \n",
        "        attack_actions = attacker_agent.act(self.states)\n",
        "        self.states = env.get_states(attack_actions)\n",
        "        \n",
        "        # Done allways false in this continuous task       \n",
        "        self.done = np.zeros(len(attack_actions),dtype=bool)\n",
        "            \n",
        "        return self.states, self.def_reward,self.att_reward, attack_actions, self.done\n",
        "    \n",
        "    '''\n",
        "    Provide the actual states for the selected attacker actions\n",
        "    Parameters:\n",
        "        self:\n",
        "        attacker_actions: optimum attacks selected by the attacker\n",
        "            it can be one of attack_names list and select random of this\n",
        "    Returns:\n",
        "        State: Actual state for the selected attacks\n",
        "    '''\n",
        "    def get_states(self,attacker_actions):\n",
        "        first = True\n",
        "        for attack in attacker_actions:\n",
        "            if first:\n",
        "                minibatch = (self.df[self.df[self.attack_names[attack]]==1].sample(1))\n",
        "                first = False\n",
        "            else:\n",
        "                minibatch=minibatch.append(self.df[self.df[self.attack_names[attack]]==1].sample(1))\n",
        "        \n",
        "        self.labels = minibatch[self.attack_names]\n",
        "        minibatch.drop(self.all_attack_names,axis=1,inplace=True)\n",
        "        self.states = minibatch\n",
        "        \n",
        "        return self.states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0eU7ZSGfNEB",
        "colab_type": "code",
        "outputId": "b0e78231-4f47-488b-c443-70c326d15e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6757
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "   \n",
        "    \n",
        "    # Train batch\n",
        "    batch_size = 1\n",
        "    # batch of memory ExpRep\n",
        "    minibatch_size = 10\n",
        "    ExpRep = True\n",
        "    \n",
        "    iterations_episode = 100\n",
        "  \n",
        "    # Initialization of the enviroment\n",
        "    env = RLenv(\"train\",batch_size=batch_size,\n",
        "                iterations_episode=iterations_episode)    \n",
        "    # obs_size = size of the state\n",
        "    obs_size = env.data_shape[1]-len(env.all_attack_names)\n",
        "    \n",
        "    #num_episodes = int(env.data_shape[0]/(iterations_episode)/10)\n",
        "    num_episodes = 100\n",
        "    \n",
        "    '''\n",
        "    Definition for the defensor agent.\n",
        "    '''\n",
        "    defender_valid_actions = list(range(len(env.attack_types))) # only detect type of attack\n",
        "    defender_num_actions = len(defender_valid_actions)    \n",
        "    \n",
        "\t\n",
        "    def_epsilon = 1 # exploration\n",
        "    min_epsilon = 0.01 # min value for exploration\n",
        "    def_gamma = 0.001\n",
        "    def_decay_rate = 0.999\n",
        "    \n",
        "    def_hidden_size = 100\n",
        "    def_hidden_layers = 2\n",
        "    \n",
        "    def_learning_rate = .01\n",
        "    \n",
        "    defender_agent = DefenderAgent(defender_valid_actions,obs_size,\"EpsilonGreedy\",\n",
        "                          epoch_length = iterations_episode,\n",
        "                          epsilon = def_epsilon,\n",
        "                          min_epsilon = min_epsilon,\n",
        "                          decay_rate = def_decay_rate,\n",
        "                          gamma = def_gamma,\n",
        "                          hidden_size=def_hidden_size,\n",
        "                          hidden_layers=def_hidden_layers,\n",
        "                          minibatch_size = minibatch_size,\n",
        "                          mem_size = 1000,\n",
        "                          learning_rate=def_learning_rate,\n",
        "                          ExpRep=ExpRep)\n",
        "    #Pretrained defender\n",
        "    #defender_agent.model_network.model.load_weights(\"models/type_model.h5\")    \n",
        "    \n",
        "    '''\n",
        "    Definition for the attacker agent.\n",
        "    In this case the exploration is better to be greater\n",
        "    The correlation sould be greater too so gamma bigger\n",
        "    '''\n",
        "    attack_valid_actions = list(range(len(env.attack_names)))\n",
        "    attack_num_actions = len(attack_valid_actions)\n",
        "\t\n",
        "    att_epsilon = 1\n",
        "    min_epsilon = 0.99 # min value for exploration\n",
        "\n",
        "    att_gamma = 0.001\n",
        "    att_decay_rate = 0.99\n",
        "    \n",
        "    att_hidden_layers = 1\n",
        "    att_hidden_size = 100\n",
        "    \n",
        "    att_learning_rate = 0.2\n",
        "    \n",
        "    attacker_agent = AttackAgent(attack_valid_actions,obs_size,\"EpsilonGreedy\",\n",
        "                          epoch_length = iterations_episode,\n",
        "                          epsilon = att_epsilon,\n",
        "                          min_epsilon = min_epsilon,\n",
        "                          decay_rate = att_decay_rate,\n",
        "                          gamma = att_gamma,\n",
        "                          hidden_size=att_hidden_size,\n",
        "                          hidden_layers=att_hidden_layers,\n",
        "                          minibatch_size = minibatch_size,\n",
        "                          mem_size = 1000,\n",
        "                          learning_rate=att_learning_rate,\n",
        "                          ExpRep=ExpRep)\n",
        "    \n",
        "        \n",
        "    \n",
        "    # Statistics\n",
        "    att_reward_chain = []\n",
        "    def_reward_chain = []\n",
        "    att_loss_chain = []\n",
        "    def_loss_chain = []\n",
        "    def_total_reward_chain = []\n",
        "    att_total_reward_chain = []\n",
        "    \n",
        "\t# Print parameters\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Total epoch: {} | Iterations in epoch: {}\"\n",
        "          \"| Minibatch from mem size: {} | Total Samples: {}|\".format(num_episodes,\n",
        "                         iterations_episode,minibatch_size,\n",
        "                         num_episodes*iterations_episode))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Dataset shape: {}\".format(env.data_shape))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Attacker parameters: Num_actions={} | gamma={} |\" \n",
        "          \" epsilon={} | ANN hidden size={} | \"\n",
        "          \"ANN hidden layers={}|\".format(attack_num_actions,\n",
        "                             att_gamma,att_epsilon, att_hidden_size,\n",
        "                             att_hidden_layers))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Defense parameters: Num_actions={} | gamma={} | \"\n",
        "          \"epsilon={} | ANN hidden size={} |\"\n",
        "          \" ANN hidden layers={}|\".format(defender_num_actions,\n",
        "                              def_gamma,def_epsilon,def_hidden_size,\n",
        "                              def_hidden_layers))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Main loop\n",
        "    attacks_by_epoch = []\n",
        "    attack_labels_list = []\n",
        "    for epoch in range(num_episodes):\n",
        "        start_time = time.time()\n",
        "        att_loss = 0.\n",
        "        def_loss = 0.\n",
        "        def_total_reward_by_episode = 0\n",
        "        att_total_reward_by_episode = 0\n",
        "        # Reset enviromet, actualize the data batch with random state/attacks\n",
        "        states = env.reset()\n",
        "        \n",
        "        # Get actions for actual states following the policy\n",
        "        attack_actions = attacker_agent.act(states)\n",
        "        states = env.get_states(attack_actions)    \n",
        "        \n",
        "        done = False\n",
        "       \n",
        "        attacks_list = []\n",
        "        # Iteration in one episode\n",
        "        for i_iteration in range(iterations_episode):\n",
        "            \n",
        "            attacks_list.append(attack_actions[0])\n",
        "            # apply actions, get rewards and new state\n",
        "            act_time = time.time()  \n",
        "            defender_actions = defender_agent.act(states)\n",
        "            #Enviroment actuation for this actions\n",
        "            next_states,def_reward, att_reward,next_attack_actions, done = env.act(defender_actions,attack_actions)\n",
        "            # If the epoch*batch_size*iterations_episode is largest than the df\n",
        "\n",
        "            \n",
        "            attacker_agent.learn(states,attack_actions,next_states,att_reward,done)\n",
        "            defender_agent.learn(states,defender_actions,next_states,def_reward,done)\n",
        "            \n",
        "            act_end_time = time.time()\n",
        "            \n",
        "            # Train network, update loss after at least minibatch_learns\n",
        "            if ExpRep and epoch*iterations_episode + i_iteration >= minibatch_size:\n",
        "                def_loss += defender_agent.update_model()\n",
        "                att_loss += attacker_agent.update_model()\n",
        "            elif not ExpRep:\n",
        "                def_loss += defender_agent.update_model()\n",
        "                att_loss += attacker_agent.update_model()\n",
        "                \n",
        "\n",
        "            update_end_time = time.time()\n",
        "\n",
        "            # Update the state\n",
        "            states = next_states\n",
        "            attack_actions = next_attack_actions\n",
        "            \n",
        "            \n",
        "            # Update statistics\n",
        "            def_total_reward_by_episode += np.sum(def_reward,dtype=np.int32)\n",
        "            att_total_reward_by_episode += np.sum(att_reward,dtype=np.int32)\n",
        "        \n",
        "        attacks_by_epoch.append(attacks_list)\n",
        "        # Update user view\n",
        "        def_reward_chain.append(def_total_reward_by_episode) \n",
        "        att_reward_chain.append(att_total_reward_by_episode) \n",
        "        def_loss_chain.append(def_loss)\n",
        "        att_loss_chain.append(att_loss) \n",
        "\n",
        "        \n",
        "        end_time = time.time()\n",
        "        print(\"\\r\\n|Epoch {:03d}/{:03d}| time: {:2.2f}|\\r\\n\"\n",
        "                \"|Def Loss {:4.4f} | Def Reward in ep {:03d}|\\r\\n\"\n",
        "                \"|Att Loss {:4.4f} | Att Reward in ep {:03d}|\"\n",
        "                .format(epoch, num_episodes,(end_time-start_time), \n",
        "                def_loss, def_total_reward_by_episode,\n",
        "                att_loss, att_total_reward_by_episode))\n",
        "        \n",
        "        \n",
        "        print(\"|Def Estimated: {}| Att Labels: {}\".format(env.def_estimated_labels,\n",
        "              env.def_true_labels))\n",
        "        attack_labels_list.append(env.def_true_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "-------------------------------------------------------------------------------\n",
            "Total epoch: 100 | Iterations in epoch: 100| Minibatch from mem size: 10 | Total Samples: 10000|\n",
            "-------------------------------------------------------------------------------\n",
            "Dataset shape: (1795575, 50)\n",
            "-------------------------------------------------------------------------------\n",
            "Attacker parameters: Num_actions=4 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=1|\n",
            "-------------------------------------------------------------------------------\n",
            "Defense parameters: Num_actions=4 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=2|\n",
            "-------------------------------------------------------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "|Epoch 000/100| time: 16.23|\n",
            "|Def Loss 0.9783 | Def Reward in ep 017|\n",
            "|Att Loss 2.1062 | Att Reward in ep 083|\n",
            "|Def Estimated: [28 21 21 30]| Att Labels: [28 22 23 27]\n",
            "\n",
            "|Epoch 001/100| time: 12.32|\n",
            "|Def Loss 0.6191 | Def Reward in ep 020|\n",
            "|Att Loss 1.5696 | Att Reward in ep 080|\n",
            "|Def Estimated: [25 25 27 23]| Att Labels: [20 20 32 28]\n",
            "\n",
            "|Epoch 002/100| time: 12.53|\n",
            "|Def Loss 0.4279 | Def Reward in ep 034|\n",
            "|Att Loss 2.2287 | Att Reward in ep 066|\n",
            "|Def Estimated: [30 26 22 22]| Att Labels: [22 25 26 27]\n",
            "\n",
            "|Epoch 003/100| time: 12.29|\n",
            "|Def Loss 0.4485 | Def Reward in ep 051|\n",
            "|Att Loss 2.5238 | Att Reward in ep 049|\n",
            "|Def Estimated: [20 27 26 27]| Att Labels: [20 29 29 22]\n",
            "\n",
            "|Epoch 004/100| time: 13.23|\n",
            "|Def Loss 0.3576 | Def Reward in ep 067|\n",
            "|Att Loss 2.7225 | Att Reward in ep 033|\n",
            "|Def Estimated: [25 23 23 29]| Att Labels: [25 23 22 30]\n",
            "\n",
            "|Epoch 005/100| time: 13.03|\n",
            "|Def Loss 0.3295 | Def Reward in ep 076|\n",
            "|Att Loss 2.9541 | Att Reward in ep 024|\n",
            "|Def Estimated: [28 27 26 19]| Att Labels: [24 25 24 27]\n",
            "\n",
            "|Epoch 006/100| time: 15.37|\n",
            "|Def Loss 0.2907 | Def Reward in ep 081|\n",
            "|Att Loss 3.0225 | Att Reward in ep 019|\n",
            "|Def Estimated: [29 27 21 23]| Att Labels: [33 22 23 22]\n",
            "\n",
            "|Epoch 007/100| time: 13.28|\n",
            "|Def Loss 0.3189 | Def Reward in ep 091|\n",
            "|Att Loss 3.0163 | Att Reward in ep 009|\n",
            "|Def Estimated: [24 30 19 27]| Att Labels: [25 28 19 28]\n",
            "\n",
            "|Epoch 008/100| time: 12.58|\n",
            "|Def Loss 0.3039 | Def Reward in ep 092|\n",
            "|Att Loss 2.9706 | Att Reward in ep 008|\n",
            "|Def Estimated: [26 21 24 29]| Att Labels: [22 25 24 29]\n",
            "\n",
            "|Epoch 009/100| time: 12.88|\n",
            "|Def Loss 0.3352 | Def Reward in ep 094|\n",
            "|Att Loss 2.8249 | Att Reward in ep 006|\n",
            "|Def Estimated: [25 24 25 26]| Att Labels: [23 25 25 27]\n",
            "\n",
            "|Epoch 010/100| time: 13.23|\n",
            "|Def Loss 0.3964 | Def Reward in ep 095|\n",
            "|Att Loss 2.7968 | Att Reward in ep 005|\n",
            "|Def Estimated: [23 24 33 20]| Att Labels: [24 26 32 18]\n",
            "\n",
            "|Epoch 011/100| time: 13.32|\n",
            "|Def Loss 0.3384 | Def Reward in ep 093|\n",
            "|Att Loss 2.1895 | Att Reward in ep 007|\n",
            "|Def Estimated: [30 20 19 31]| Att Labels: [25 21 18 36]\n",
            "\n",
            "|Epoch 012/100| time: 14.43|\n",
            "|Def Loss 0.2980 | Def Reward in ep 093|\n",
            "|Att Loss 1.6576 | Att Reward in ep 007|\n",
            "|Def Estimated: [29 21 27 23]| Att Labels: [29 20 27 24]\n",
            "\n",
            "|Epoch 013/100| time: 13.29|\n",
            "|Def Loss 0.2694 | Def Reward in ep 097|\n",
            "|Att Loss 1.1564 | Att Reward in ep 003|\n",
            "|Def Estimated: [24 29 22 25]| Att Labels: [25 28 22 25]\n",
            "\n",
            "|Epoch 014/100| time: 14.34|\n",
            "|Def Loss 0.2990 | Def Reward in ep 095|\n",
            "|Att Loss 0.7028 | Att Reward in ep 005|\n",
            "|Def Estimated: [28 25 23 24]| Att Labels: [28 26 23 23]\n",
            "\n",
            "|Epoch 015/100| time: 15.18|\n",
            "|Def Loss 0.3461 | Def Reward in ep 090|\n",
            "|Att Loss 0.6156 | Att Reward in ep 010|\n",
            "|Def Estimated: [32 20 23 25]| Att Labels: [33 20 23 24]\n",
            "\n",
            "|Epoch 016/100| time: 12.73|\n",
            "|Def Loss 0.2210 | Def Reward in ep 094|\n",
            "|Att Loss 0.4420 | Att Reward in ep 006|\n",
            "|Def Estimated: [18 22 34 26]| Att Labels: [22 24 34 20]\n",
            "\n",
            "|Epoch 017/100| time: 13.79|\n",
            "|Def Loss 0.2466 | Def Reward in ep 093|\n",
            "|Att Loss 0.2462 | Att Reward in ep 007|\n",
            "|Def Estimated: [28 25 23 24]| Att Labels: [27 28 23 22]\n",
            "\n",
            "|Epoch 018/100| time: 12.33|\n",
            "|Def Loss 0.2156 | Def Reward in ep 095|\n",
            "|Att Loss 0.2329 | Att Reward in ep 005|\n",
            "|Def Estimated: [22 30 24 24]| Att Labels: [21 30 24 25]\n",
            "\n",
            "|Epoch 019/100| time: 14.15|\n",
            "|Def Loss 0.2730 | Def Reward in ep 095|\n",
            "|Att Loss 0.2508 | Att Reward in ep 005|\n",
            "|Def Estimated: [27 25 26 22]| Att Labels: [27 26 25 22]\n",
            "\n",
            "|Epoch 020/100| time: 13.37|\n",
            "|Def Loss 0.2446 | Def Reward in ep 098|\n",
            "|Att Loss 0.3034 | Att Reward in ep 002|\n",
            "|Def Estimated: [27 21 29 23]| Att Labels: [25 22 29 24]\n",
            "\n",
            "|Epoch 021/100| time: 10.57|\n",
            "|Def Loss 0.1843 | Def Reward in ep 097|\n",
            "|Att Loss 0.2379 | Att Reward in ep 003|\n",
            "|Def Estimated: [15 25 27 33]| Att Labels: [14 26 27 33]\n",
            "\n",
            "|Epoch 022/100| time: 14.35|\n",
            "|Def Loss 0.2565 | Def Reward in ep 097|\n",
            "|Att Loss 0.2429 | Att Reward in ep 003|\n",
            "|Def Estimated: [27 24 21 28]| Att Labels: [28 24 21 27]\n",
            "\n",
            "|Epoch 023/100| time: 15.10|\n",
            "|Def Loss 0.1772 | Def Reward in ep 093|\n",
            "|Att Loss 0.2898 | Att Reward in ep 007|\n",
            "|Def Estimated: [31 30 19 20]| Att Labels: [32 28 20 20]\n",
            "\n",
            "|Epoch 024/100| time: 11.79|\n",
            "|Def Loss 0.3200 | Def Reward in ep 093|\n",
            "|Att Loss 0.1843 | Att Reward in ep 007|\n",
            "|Def Estimated: [19 26 30 25]| Att Labels: [19 25 29 27]\n",
            "\n",
            "|Epoch 025/100| time: 12.92|\n",
            "|Def Loss 0.1877 | Def Reward in ep 094|\n",
            "|Att Loss 0.1436 | Att Reward in ep 006|\n",
            "|Def Estimated: [24 33 26 17]| Att Labels: [23 31 27 19]\n",
            "\n",
            "|Epoch 026/100| time: 12.62|\n",
            "|Def Loss 0.2372 | Def Reward in ep 093|\n",
            "|Att Loss 0.2354 | Att Reward in ep 007|\n",
            "|Def Estimated: [24 26 29 21]| Att Labels: [21 26 28 25]\n",
            "\n",
            "|Epoch 027/100| time: 12.07|\n",
            "|Def Loss 0.2514 | Def Reward in ep 095|\n",
            "|Att Loss 0.1679 | Att Reward in ep 005|\n",
            "|Def Estimated: [21 25 27 27]| Att Labels: [20 25 27 28]\n",
            "\n",
            "|Epoch 028/100| time: 12.80|\n",
            "|Def Loss 0.2527 | Def Reward in ep 098|\n",
            "|Att Loss 0.1794 | Att Reward in ep 002|\n",
            "|Def Estimated: [25 39 18 18]| Att Labels: [23 40 18 19]\n",
            "\n",
            "|Epoch 029/100| time: 13.53|\n",
            "|Def Loss 0.2418 | Def Reward in ep 095|\n",
            "|Att Loss 0.2474 | Att Reward in ep 005|\n",
            "|Def Estimated: [30 19 20 31]| Att Labels: [26 22 21 31]\n",
            "\n",
            "|Epoch 030/100| time: 13.59|\n",
            "|Def Loss 0.2197 | Def Reward in ep 096|\n",
            "|Att Loss 0.2466 | Att Reward in ep 004|\n",
            "|Def Estimated: [28 23 21 28]| Att Labels: [25 25 21 29]\n",
            "\n",
            "|Epoch 031/100| time: 12.12|\n",
            "|Def Loss 0.2031 | Def Reward in ep 098|\n",
            "|Att Loss 0.1518 | Att Reward in ep 002|\n",
            "|Def Estimated: [21 27 31 21]| Att Labels: [19 27 31 23]\n",
            "\n",
            "|Epoch 032/100| time: 12.37|\n",
            "|Def Loss 0.1552 | Def Reward in ep 096|\n",
            "|Att Loss 0.2677 | Att Reward in ep 004|\n",
            "|Def Estimated: [22 25 27 26]| Att Labels: [21 25 27 27]\n",
            "\n",
            "|Epoch 033/100| time: 12.32|\n",
            "|Def Loss 0.2417 | Def Reward in ep 098|\n",
            "|Att Loss 0.2167 | Att Reward in ep 002|\n",
            "|Def Estimated: [20 24 25 31]| Att Labels: [20 23 25 32]\n",
            "\n",
            "|Epoch 034/100| time: 13.64|\n",
            "|Def Loss 0.1671 | Def Reward in ep 097|\n",
            "|Att Loss 0.2242 | Att Reward in ep 003|\n",
            "|Def Estimated: [25 26 27 22]| Att Labels: [25 27 27 21]\n",
            "\n",
            "|Epoch 035/100| time: 12.06|\n",
            "|Def Loss 0.1217 | Def Reward in ep 098|\n",
            "|Att Loss 0.0993 | Att Reward in ep 002|\n",
            "|Def Estimated: [22 27 26 25]| Att Labels: [20 28 26 26]\n",
            "\n",
            "|Epoch 036/100| time: 12.86|\n",
            "|Def Loss 0.1762 | Def Reward in ep 097|\n",
            "|Att Loss 0.1986 | Att Reward in ep 003|\n",
            "|Def Estimated: [24 25 27 24]| Att Labels: [23 27 28 22]\n",
            "\n",
            "|Epoch 037/100| time: 13.37|\n",
            "|Def Loss 0.1302 | Def Reward in ep 095|\n",
            "|Att Loss 0.2160 | Att Reward in ep 005|\n",
            "|Def Estimated: [25 22 28 25]| Att Labels: [25 22 28 25]\n",
            "\n",
            "|Epoch 038/100| time: 13.72|\n",
            "|Def Loss 0.1499 | Def Reward in ep 098|\n",
            "|Att Loss 0.1584 | Att Reward in ep 002|\n",
            "|Def Estimated: [27 33 17 23]| Att Labels: [27 32 17 24]\n",
            "\n",
            "|Epoch 039/100| time: 13.99|\n",
            "|Def Loss 0.1178 | Def Reward in ep 098|\n",
            "|Att Loss 0.1678 | Att Reward in ep 002|\n",
            "|Def Estimated: [26 26 26 22]| Att Labels: [28 24 26 22]\n",
            "\n",
            "|Epoch 040/100| time: 12.33|\n",
            "|Def Loss 0.1513 | Def Reward in ep 096|\n",
            "|Att Loss 0.1757 | Att Reward in ep 004|\n",
            "|Def Estimated: [23 30 17 30]| Att Labels: [22 32 17 29]\n",
            "\n",
            "|Epoch 041/100| time: 13.54|\n",
            "|Def Loss 0.1784 | Def Reward in ep 096|\n",
            "|Att Loss 0.1495 | Att Reward in ep 004|\n",
            "|Def Estimated: [24 26 27 23]| Att Labels: [26 26 25 23]\n",
            "\n",
            "|Epoch 042/100| time: 14.39|\n",
            "|Def Loss 0.1477 | Def Reward in ep 096|\n",
            "|Att Loss 0.1471 | Att Reward in ep 004|\n",
            "|Def Estimated: [27 20 27 26]| Att Labels: [29 21 27 23]\n",
            "\n",
            "|Epoch 043/100| time: 14.86|\n",
            "|Def Loss 0.2232 | Def Reward in ep 097|\n",
            "|Att Loss 0.1787 | Att Reward in ep 003|\n",
            "|Def Estimated: [29 20 19 32]| Att Labels: [30 20 19 31]\n",
            "\n",
            "|Epoch 044/100| time: 13.32|\n",
            "|Def Loss 0.2060 | Def Reward in ep 097|\n",
            "|Att Loss 0.1751 | Att Reward in ep 003|\n",
            "|Def Estimated: [25 25 23 27]| Att Labels: [25 24 23 28]\n",
            "\n",
            "|Epoch 045/100| time: 12.51|\n",
            "|Def Loss 0.1242 | Def Reward in ep 098|\n",
            "|Att Loss 0.1504 | Att Reward in ep 002|\n",
            "|Def Estimated: [23 31 20 26]| Att Labels: [22 31 20 27]\n",
            "\n",
            "|Epoch 046/100| time: 14.34|\n",
            "|Def Loss 0.1390 | Def Reward in ep 096|\n",
            "|Att Loss 0.2082 | Att Reward in ep 004|\n",
            "|Def Estimated: [28 19 26 27]| Att Labels: [29 17 26 28]\n",
            "\n",
            "|Epoch 047/100| time: 14.41|\n",
            "|Def Loss 0.2182 | Def Reward in ep 097|\n",
            "|Att Loss 0.1475 | Att Reward in ep 003|\n",
            "|Def Estimated: [26 27 23 24]| Att Labels: [29 25 23 23]\n",
            "\n",
            "|Epoch 048/100| time: 12.30|\n",
            "|Def Loss 0.1726 | Def Reward in ep 099|\n",
            "|Att Loss 0.1138 | Att Reward in ep 001|\n",
            "|Def Estimated: [20 26 26 28]| Att Labels: [20 25 27 28]\n",
            "\n",
            "|Epoch 049/100| time: 14.59|\n",
            "|Def Loss 0.1163 | Def Reward in ep 095|\n",
            "|Att Loss 0.1623 | Att Reward in ep 005|\n",
            "|Def Estimated: [25 27 23 25]| Att Labels: [29 26 22 23]\n",
            "\n",
            "|Epoch 050/100| time: 13.07|\n",
            "|Def Loss 0.1718 | Def Reward in ep 095|\n",
            "|Att Loss 0.1665 | Att Reward in ep 005|\n",
            "|Def Estimated: [25 26 21 28]| Att Labels: [23 29 20 28]\n",
            "\n",
            "|Epoch 051/100| time: 12.60|\n",
            "|Def Loss 0.1461 | Def Reward in ep 098|\n",
            "|Att Loss 0.1933 | Att Reward in ep 002|\n",
            "|Def Estimated: [21 16 31 32]| Att Labels: [22 16 31 31]\n",
            "\n",
            "|Epoch 052/100| time: 12.68|\n",
            "|Def Loss 0.1688 | Def Reward in ep 098|\n",
            "|Att Loss 0.1509 | Att Reward in ep 002|\n",
            "|Def Estimated: [23 21 35 21]| Att Labels: [22 23 35 20]\n",
            "\n",
            "|Epoch 053/100| time: 15.33|\n",
            "|Def Loss 0.1924 | Def Reward in ep 099|\n",
            "|Att Loss 0.1054 | Att Reward in ep 001|\n",
            "|Def Estimated: [31 26 23 20]| Att Labels: [32 25 23 20]\n",
            "\n",
            "|Epoch 054/100| time: 14.29|\n",
            "|Def Loss 0.1785 | Def Reward in ep 095|\n",
            "|Att Loss 0.1430 | Att Reward in ep 005|\n",
            "|Def Estimated: [30 27 22 21]| Att Labels: [29 31 22 18]\n",
            "\n",
            "|Epoch 055/100| time: 12.46|\n",
            "|Def Loss 0.1600 | Def Reward in ep 097|\n",
            "|Att Loss 0.2334 | Att Reward in ep 003|\n",
            "|Def Estimated: [23 31 19 27]| Att Labels: [22 31 20 27]\n",
            "\n",
            "|Epoch 056/100| time: 14.91|\n",
            "|Def Loss 0.1730 | Def Reward in ep 097|\n",
            "|Att Loss 0.1299 | Att Reward in ep 003|\n",
            "|Def Estimated: [31 23 29 17]| Att Labels: [30 24 29 17]\n",
            "\n",
            "|Epoch 057/100| time: 12.12|\n",
            "|Def Loss 0.0999 | Def Reward in ep 099|\n",
            "|Att Loss 0.2146 | Att Reward in ep 001|\n",
            "|Def Estimated: [20 21 34 25]| Att Labels: [20 22 33 25]\n",
            "\n",
            "|Epoch 058/100| time: 14.05|\n",
            "|Def Loss 0.1089 | Def Reward in ep 099|\n",
            "|Att Loss 0.1575 | Att Reward in ep 001|\n",
            "|Def Estimated: [29 24 30 17]| Att Labels: [28 25 30 17]\n",
            "\n",
            "|Epoch 059/100| time: 14.55|\n",
            "|Def Loss 0.1122 | Def Reward in ep 099|\n",
            "|Att Loss 0.1915 | Att Reward in ep 001|\n",
            "|Def Estimated: [31 27 24 18]| Att Labels: [30 28 24 18]\n",
            "\n",
            "|Epoch 060/100| time: 13.43|\n",
            "|Def Loss 0.0993 | Def Reward in ep 096|\n",
            "|Att Loss 0.1539 | Att Reward in ep 004|\n",
            "|Def Estimated: [26 28 19 27]| Att Labels: [25 30 19 26]\n",
            "\n",
            "|Epoch 061/100| time: 13.07|\n",
            "|Def Loss 0.0797 | Def Reward in ep 097|\n",
            "|Att Loss 0.1808 | Att Reward in ep 003|\n",
            "|Def Estimated: [24 23 19 34]| Att Labels: [25 25 18 32]\n",
            "\n",
            "|Epoch 062/100| time: 13.03|\n",
            "|Def Loss 0.0726 | Def Reward in ep 096|\n",
            "|Att Loss 0.1414 | Att Reward in ep 004|\n",
            "|Def Estimated: [21 26 26 27]| Att Labels: [24 25 25 26]\n",
            "\n",
            "|Epoch 063/100| time: 13.77|\n",
            "|Def Loss 0.1104 | Def Reward in ep 096|\n",
            "|Att Loss 0.1980 | Att Reward in ep 004|\n",
            "|Def Estimated: [23 17 30 30]| Att Labels: [25 17 30 28]\n",
            "\n",
            "|Epoch 064/100| time: 13.93|\n",
            "|Def Loss 0.1076 | Def Reward in ep 097|\n",
            "|Att Loss 0.1801 | Att Reward in ep 003|\n",
            "|Def Estimated: [26 24 20 30]| Att Labels: [27 23 20 30]\n",
            "\n",
            "|Epoch 065/100| time: 16.09|\n",
            "|Def Loss 0.0853 | Def Reward in ep 095|\n",
            "|Att Loss 0.0909 | Att Reward in ep 005|\n",
            "|Def Estimated: [37 19 24 20]| Att Labels: [34 24 24 18]\n",
            "\n",
            "|Epoch 066/100| time: 14.62|\n",
            "|Def Loss 0.1260 | Def Reward in ep 097|\n",
            "|Att Loss 0.0505 | Att Reward in ep 003|\n",
            "|Def Estimated: [28 34 13 25]| Att Labels: [29 34 13 24]\n",
            "\n",
            "|Epoch 067/100| time: 12.97|\n",
            "|Def Loss 0.1363 | Def Reward in ep 097|\n",
            "|Att Loss 0.0635 | Att Reward in ep 003|\n",
            "|Def Estimated: [22 19 29 30]| Att Labels: [23 19 29 29]\n",
            "\n",
            "|Epoch 068/100| time: 14.26|\n",
            "|Def Loss 0.1357 | Def Reward in ep 099|\n",
            "|Att Loss 0.0863 | Att Reward in ep 001|\n",
            "|Def Estimated: [29 25 24 22]| Att Labels: [28 26 24 22]\n",
            "\n",
            "|Epoch 069/100| time: 12.30|\n",
            "|Def Loss 0.1141 | Def Reward in ep 100|\n",
            "|Att Loss 0.0390 | Att Reward in ep 000|\n",
            "|Def Estimated: [19 25 34 22]| Att Labels: [19 25 34 22]\n",
            "\n",
            "|Epoch 070/100| time: 13.70|\n",
            "|Def Loss 0.1884 | Def Reward in ep 095|\n",
            "|Att Loss 0.1155 | Att Reward in ep 005|\n",
            "|Def Estimated: [25 31 22 22]| Att Labels: [26 32 21 21]\n",
            "\n",
            "|Epoch 071/100| time: 12.83|\n",
            "|Def Loss 0.1858 | Def Reward in ep 097|\n",
            "|Att Loss 0.1070 | Att Reward in ep 003|\n",
            "|Def Estimated: [24 27 17 32]| Att Labels: [22 28 17 33]\n",
            "\n",
            "|Epoch 072/100| time: 12.19|\n",
            "|Def Loss 0.1576 | Def Reward in ep 098|\n",
            "|Att Loss 0.1302 | Att Reward in ep 002|\n",
            "|Def Estimated: [21 28 31 20]| Att Labels: [20 29 30 21]\n",
            "\n",
            "|Epoch 073/100| time: 12.45|\n",
            "|Def Loss 0.1041 | Def Reward in ep 094|\n",
            "|Att Loss 0.1813 | Att Reward in ep 006|\n",
            "|Def Estimated: [19 26 30 25]| Att Labels: [21 25 30 24]\n",
            "\n",
            "|Epoch 074/100| time: 12.36|\n",
            "|Def Loss 0.1023 | Def Reward in ep 100|\n",
            "|Att Loss 0.1154 | Att Reward in ep 000|\n",
            "|Def Estimated: [20 27 21 32]| Att Labels: [20 27 21 32]\n",
            "\n",
            "|Epoch 075/100| time: 12.62|\n",
            "|Def Loss 0.1293 | Def Reward in ep 098|\n",
            "|Att Loss 0.1127 | Att Reward in ep 002|\n",
            "|Def Estimated: [21 24 27 28]| Att Labels: [22 23 26 29]\n",
            "\n",
            "|Epoch 076/100| time: 13.51|\n",
            "|Def Loss 0.1030 | Def Reward in ep 096|\n",
            "|Att Loss 0.1859 | Att Reward in ep 004|\n",
            "|Def Estimated: [24 34 25 17]| Att Labels: [24 35 24 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w56BPg3zo0DC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2QzwrilomWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoBlm8f7fW_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "# Save trained model weights and architecture, used in test\n",
        "defender_agent.model_network.model.save_weights(\"models/defender_agent_model.h5\", overwrite=True)\n",
        "with open(\"models/defender_agent_model.json\", \"w\") as outfile:\n",
        "    json.dump(defender_agent.model_network.model.to_json(), outfile)\n",
        "\n",
        "\n",
        "    \n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')    \n",
        "# Plot training results\n",
        "plt.figure(1)\n",
        "plt.subplot(211)\n",
        "plt.plot(np.arange(len(def_reward_chain)),def_reward_chain,label='Defense')\n",
        "plt.plot(np.arange(len(att_reward_chain)),att_reward_chain,label='Attack')\n",
        "plt.title('Total reward by episode')\n",
        "plt.xlabel('n Episode')\n",
        "plt.ylabel('Total reward')\n",
        "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(np.arange(len(def_loss_chain)),def_loss_chain,label='Defense')\n",
        "plt.plot(np.arange(len(att_loss_chain)),att_loss_chain,label='Attack')\n",
        "plt.title('Loss by episode')\n",
        "plt.xlabel('n Episode')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "#plt.show()\n",
        "plt.savefig('results/train_adv.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHpLDW9kfeQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bins=np.arange(5) -0.5\n",
        "# Plot attacks distribution alongside\n",
        "plt.figure(2,figsize=[12,5])\n",
        "plt.title(\"Attacks distribution throughout  episodes\")\n",
        "for indx,e in enumerate([0,70,90]):\n",
        "    plt.subplot(1,3,indx+1)\n",
        "    plt.hist(attacks_by_epoch[e],bins=bins,width=0.9,align='left')\n",
        "    plt.xlabel(\"{} epoch\".format(e))\n",
        "    plt.xticks(bins, env.attack_names, rotation=90)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/Attacks_distribution.svg', format='svg', dpi=1000)\n",
        "\n",
        "\n",
        " # Plot attacks distribution alongside\n",
        "plt.figure(3,figsize=[10,10])\n",
        "plt.title(\"Attacks (mapped) distribution throughout  episodes\")\n",
        "for indx,e in enumerate([0,10,20,30,40,60,70,80,90]):\n",
        "    plt.subplot(3,3,indx+1)\n",
        "    plt.bar(range(4),attack_labels_list[e],tick_label =['normal', 'flooding', 'injection', 'impersonation'])\n",
        "    plt.xlabel(\"{} epoch\".format(e))\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/Attacks_mapped_distribution.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GdEEgnejoANJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MLhFt4v2oA5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3z12k-CoBqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ]
    },
    {
      "metadata": {
        "id": "k2EScB5ZoA8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import  confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1PgEGysoOlU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.grid(None)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cSeFJb3oPVT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"models/defender_agent_model.json\", \"r\") as jfile:\n",
        "    model = model_from_json(json.load(jfile))\n",
        "model.load_weights(\"models/defender_agent_model.h5\")\n",
        "\n",
        "model.compile(loss=huber_loss,optimizer=\"sgd\")\n",
        "\n",
        "\n",
        "# Define environment, game, make sure the batch_size is the same in train\n",
        "env_test = RLenv('test')\n",
        "\n",
        "\n",
        "total_reward = 0    \n",
        "\n",
        "\n",
        "true_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "estimated_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "estimated_correct_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "\n",
        "#states , labels = env.get_sequential_batch(test_path,batch_size = env.batch_size)\n",
        "states , labels = env_test.get_full()\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "q = model.predict(states)\n",
        "actions = np.argmax(q,axis=1)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hFU1HF1oYFG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maped=[]\n",
        "for indx,label in labels.iterrows():\n",
        "    maped.append(env_test.attack_types.index(env_test.attack_map[label.idxmax()]))\n",
        "\n",
        "labels,counts = np.unique(maped,return_counts=True)\n",
        "true_labels[labels] += counts\n",
        "\n",
        "\n",
        "\n",
        "for indx,a in enumerate(actions):\n",
        "    estimated_labels[a] +=1              \n",
        "    if a == maped[indx]:\n",
        "        total_reward += 1\n",
        "        estimated_correct_labels[a] += 1\n",
        "\n",
        "\n",
        "action_dummies = pd.get_dummies(actions)\n",
        "posible_actions = np.arange(len(env_test.attack_types))\n",
        "for non_existing_action in posible_actions:\n",
        "    if non_existing_action not in action_dummies.columns:\n",
        "        action_dummies[non_existing_action] = np.uint8(0)\n",
        "labels_dummies = pd.get_dummies(maped)\n",
        "\n",
        "normal_f1_score = f1_score(labels_dummies[0].values,action_dummies[0].values)\n",
        "flooding_f1_score = f1_score(labels_dummies[1].values,action_dummies[1].values)\n",
        "injection_f1_score = f1_score(labels_dummies[2].values,action_dummies[2].values)\n",
        "impersonation_f1_score = f1_score(labels_dummies[3].values,action_dummies[3].values)\n",
        "    \n",
        "\n",
        "Accuracy = [normal_f1_score,flooding_f1_score,injection_f1_score,impersonation_f1_score]\n",
        "Mismatch = estimated_labels - true_labels\n",
        "\n",
        "acc = float(100*total_reward/len(states))\n",
        "print('\\r\\nTotal reward: {} | Number of samples: {} | Accuracy = {:.2f}%'.format(total_reward,\n",
        "      len(states),acc))\n",
        "outputs_df = pd.DataFrame(index = env_test.attack_types,columns = [\"Estimated\",\"Correct\",\"Total\",\"F1_score\"])\n",
        "for indx,att in enumerate(env_test.attack_types):\n",
        "   outputs_df.iloc[indx].Estimated = estimated_labels[indx]\n",
        "   outputs_df.iloc[indx].Correct = estimated_correct_labels[indx]\n",
        "   outputs_df.iloc[indx].Total = true_labels[indx]\n",
        "   outputs_df.iloc[indx].F1_score = Accuracy[indx]*100\n",
        "   outputs_df.iloc[indx].Mismatch = abs(Mismatch[indx])\n",
        "    \n",
        "    \n",
        "print(outputs_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBxETWRcocbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(10, 6)\n",
        "width = 0.35\n",
        "pos = np.arange(len(true_labels))\n",
        "p1 = plt.bar(pos, estimated_correct_labels,width,color='g')\n",
        "p1 = plt.bar(pos+width,\n",
        "             (np.abs(estimated_correct_labels-true_labels)),width,\n",
        "             color='r')\n",
        "p2 = plt.bar(pos+width,np.abs(estimated_labels-estimated_correct_labels),width,\n",
        "             bottom=(np.abs(estimated_correct_labels-true_labels)),\n",
        "             color='b')\n",
        "\n",
        "ax.yaxis.set_tick_params(labelsize=15)\n",
        "ax.set_xticks(pos+width/2)\n",
        "ax.set_xticklabels(env.attack_types,rotation='vertical',fontsize = 'xx-large')\n",
        "\n",
        "#ax.set_yscale('log')\n",
        "\n",
        "#ax.set_ylim([0, 100])\n",
        "#ax.set_title('Test set scores',fontsize = 'xx-large')\n",
        "#ax.set_title('Test set scores, Acc = {:.2f}'.format(acc))\n",
        "plt.legend(('Correct estimated','False negative','False positive'),fontsize = 'x-large')\n",
        "plt.tight_layout()\n",
        "#plt.show()\n",
        "plt.savefig('results/test_adv_imp.tif', format='tif', dpi=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBVTk9aujCuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimated_correct_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UphioXJOjSpn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimated_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XrjAwLzjbWf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jw9xTn9EkMku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.abs(estimated_correct_labels-true_labels) # false negative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSneeTpxkYiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.abs(estimated_labels-estimated_correct_labels)  #false positive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0Z8IHZhonlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "aggregated_data_test = np.array(maped)\n",
        "\n",
        "print('Performance measures on Test data')\n",
        "print('Accuracy =  {:.4f}'.format(accuracy_score( aggregated_data_test,actions)))\n",
        "print('F1 =  {:.4f}'.format(f1_score(aggregated_data_test,actions, average='weighted')))\n",
        "print('Precision_score =  {:.4f}'.format(precision_score(aggregated_data_test,actions, average='weighted')))\n",
        "print('recall_score =  {:.4f}'.format(recall_score(aggregated_data_test,actions, average='weighted')))\n",
        "\n",
        "cnf_matrix = confusion_matrix(aggregated_data_test,actions)\n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure()\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=env.attack_types, normalize=True,\n",
        "                      title='AE-RL')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix_adversarial.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVn1_CP5AQtN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(aggregated_data_test,actions,target_names=['normal', 'flooding', 'injection', 'impersonation']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60L3BNptopsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#['normal', 'flooding', 'injection', 'impersonation']\n",
        "mapa = {0:'normal', 1:'flooding', 2:'injection',3:'impersonation'}\n",
        "yt_app = pd.Series(maped).map(mapa)\n",
        "\n",
        "perf_per_class = pd.DataFrame(index=range(len(yt_app.unique())),columns=['name', 'acc','f1', 'pre','rec'])\n",
        "for i,x in enumerate(pd.Series(yt_app).value_counts().index):\n",
        "    y_test_hat_check = pd.Series(actions).map(mapa).copy()\n",
        "    y_test_hat_check[y_test_hat_check != x] = 'OTHER'\n",
        "    yt_app = pd.Series(maped).map(mapa).copy()\n",
        "    yt_app[yt_app != x] = 'OTHER'\n",
        "    ac=accuracy_score( yt_app,y_test_hat_check)\n",
        "    f1=f1_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    pr=precision_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    re=recall_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    perf_per_class.iloc[i]=[x,ac,f1,pr,re]\n",
        "    \n",
        "print(\"\\r\\nOne vs All metrics: \\r\\n{}\".format(perf_per_class))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P89yhncYZ2b3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras MLP"
      ]
    },
    {
      "metadata": {
        "id": "8_q43FU0hC44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mGfiCMaAfqw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train['labels']\n",
        "x = train.drop(['labels'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eZKJF_HSBsDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwUtLgxrB-5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "03YTiDfWA85o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQ-5Xw3NFCcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AYAkgsoMCN9K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100,input_shape=(x.shape[1],) ))\n",
        "model.add(Dense(100))\n",
        "model.add(Dense(y.shape[1]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwAivdIPCKTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='mse',metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmdrxd_CKWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x=x,y=y,epochs=2,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Qg2-CxnCKYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = test['labels']\n",
        "x_test = test.drop(['labels'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcLte5bMYfte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UTk-XbgZPal",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrDuqJK7ZRi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_e = le.fit_transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xieVctcdETi2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x=x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3cgcBM6h4CP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results really overfitted"
      ]
    },
    {
      "metadata": {
        "id": "mOPHm1bpEnnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_e,np.argmax(preds,axis=-1),target_names=le.classes_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O35CUVO0ETeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Performance measures on Test data')\n",
        "print('Accuracy =  {:.4f}'.format(accuracy_score(y_test_e,np.argmax(preds,axis=-1))))\n",
        "print('F1 =  {:.4f}'.format(f1_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('Precision_score =  {:.4f}'.format(precision_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('recall_score =  {:.4f}'.format(recall_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32ae17RjfTnq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SblH6mtv8ts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "order = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "permute_dict = {0:1,1:3,2:2,3:0}\n",
        "\n",
        "\n",
        "y_test_e_permuted = np.vectorize(permute_dict.get)(y_test_e)\n",
        "predictions_permuted = np.vectorize(permute_dict.get)(np.argmax(preds,axis=-1))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test_e_permuted,predictions_permuted)\n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=order, normalize=True,\n",
        "                      title='MLP Results')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix_keras.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pb8qsGN64O8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results with random batches"
      ]
    },
    {
      "metadata": {
        "id": "MRhmq07u36Ag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3usty8c7ETao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100,input_shape=(x.shape[1],) ))\n",
        "model.add(Dense(100))\n",
        "model.add(Dense(y.shape[1]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1Fr1k2D4NFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='mse',metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGqfAVcR4m-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x=x,y=y,steps_per_epoch=100,epochs=5,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JD_6eSE5MLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x=x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gKzuojz4n-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_e,np.argmax(preds,axis=-1),target_names=le.classes_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LhKonA2L4n7-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Performance measures on Test data')\n",
        "print('Accuracy =  {:.4f}'.format(accuracy_score(y_test_e,np.argmax(preds,axis=-1))))\n",
        "print('F1 =  {:.4f}'.format(f1_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('Precision_score =  {:.4f}'.format(precision_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('recall_score =  {:.4f}'.format(recall_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHMTgnDu4n5n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "order = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "permute_dict = {0:1,1:3,2:2,3:0}\n",
        "\n",
        "\n",
        "y_test_e_permuted = np.vectorize(permute_dict.get)(y_test_e)\n",
        "predictions_permuted = np.vectorize(permute_dict.get)(np.argmax(preds,axis=-1))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test_e_permuted,predictions_permuted)\n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=le.classes_, normalize=True,\n",
        "                      title='MLP Results')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix_keras2.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lf0sZoGtwJij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SMOTE"
      ]
    },
    {
      "metadata": {
        "id": "GGkLioZb5V2Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkD5JiJn2rtL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling  import ClusterCentroids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hX1orkXqWsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x.values.shape)\n",
        "print(y.values.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bebq5ryd36Ph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_resampled, y_resampled = SMOTE().fit_resample(x.values, y.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LgQE-igV4vED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = np.random.randint(0,y_resampled.shape[0],1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OORAiqI-3pmw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sub_X = X_resampled[indices]\n",
        "sub_y = y_resampled[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sw-0CkqP4RES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sub_y.sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyaQnAkF36NI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5IpkJNHfumur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEmrPc73xf2F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_resampled.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IeeG-QVR36KN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100,input_shape=(sub_X.shape[1],) ))\n",
        "model.add(Dense(100))\n",
        "model.add(Dense(sub_y.shape[1]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kyi07IbS36FA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=huber_loss,metrics=['acc'])\n",
        "model.fit(x=sub_X,y=sub_y,steps_per_epoch=100,epochs=20,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8EL0cH1vvg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaMpY1eavg7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x=x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svf56Pkyvg4q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_e,np.argmax(preds,axis=-1),target_names=le.classes_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAfWOv4nvg2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Performance measures on Test data')\n",
        "print('Accuracy =  {:.4f}'.format(accuracy_score(y_test_e,np.argmax(preds,axis=-1))))\n",
        "print('F1 =  {:.4f}'.format(f1_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('Precision_score =  {:.4f}'.format(precision_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))\n",
        "print('recall_score =  {:.4f}'.format(recall_score(y_test_e,np.argmax(preds,axis=-1), average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMht0Ud8vgz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "order = ['normal', 'flooding', 'injection', 'impersonation']\n",
        "permute_dict = {0:1,1:3,2:2,3:0}\n",
        "\n",
        "\n",
        "y_test_e_permuted = np.vectorize(permute_dict.get)(y_test_e)\n",
        "predictions_permuted = np.vectorize(permute_dict.get)(np.argmax(preds,axis=-1))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test_e_permuted,predictions_permuted)\n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=le.classes_, normalize=True,\n",
        "                      title='Over Sampling')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix_keras_OverSampl.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZtSVYZjsmfM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data distribution"
      ]
    },
    {
      "metadata": {
        "id": "2k3OXg5J4NCp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['labels'].groupby(train['labels']).count().plot(kind=\"bar\",figsize=[10,8],rot=33,title=\"Train set\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/train_distributio.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWM7YwB54M7q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test['labels'].groupby(test['labels']).count().plot(kind=\"bar\",figsize=[10,8],rot=33,title=\"Test set\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/test_distributio.tif', format='tif', dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGEi8G53mjjU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save figures for download"
      ]
    },
    {
      "metadata": {
        "id": "zBwYXT0AmrG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!zip results.zip results/*"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}